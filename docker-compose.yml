# docker-compose.yml
name: smart_stack

services:
  ollama:
    image: ollama/ollama:latest
    container_name: smart_ollama
    volumes:
      - ollama_data:/root/.ollama
      - ./:/workspace:ro       # to access your Modelfile from the repo root
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "sh -lc 'ollama list >/dev/null 2>&1 || exit 1'"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

  # One-time init to ensure the custom model exists inside Ollama
  ollama_init:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      OLLAMA_HOST: "http://ollama:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./:/workspace:ro
    entrypoint: ["sh", "-lc"]
    command: |
      set -e
      # wait up to ~120s for API
      for _ in $(seq 1 60); do
        if ollama list >/dev/null 2>&1; then break; fi
        echo "Waiting for Ollama..."; sleep 2
      done
      test -f /workspace/ollama-smart-compiler-Modelfile
      ollama pull llama3.1
      ollama show llama3.1-smart-compiler >/dev/null 2>&1 || \
        ollama create llama3.1-smart-compiler -f /workspace/ollama-smart-compiler-Modelfile
    restart: "no"

  server:
    container_name: smart_server
    build:
      context: .
      dockerfile: Dockerfile
    image: smart_server:latest
    environment:
      MCP_SERVER_HOST: "0.0.0.0"
      MCP_SERVER_PORT: "8000"
      OLLAMA_HOST: "http://ollama:11434"
      OLLAMA_MODEL: "llama3.1:latest"
      MCP_SERVER_TRANSPORT: "sse"
      LOG_LEVEL: "INFO"
      ENABLE_REST_API: "true"
      ALLOWED_PATHS: "/app/runtime-path"
    ports:
      - "8000:8000"
    volumes:
      - server_runtime:/app/runtime-path
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
      ollama_init:
        condition: service_completed_successfully
    healthcheck:
      # Simple TCP probe
      test: ["CMD-SHELL", "python -c 'import socket,sys; s=socket.socket(); s.settimeout(2); s.connect((\"127.0.0.1\",8000)); s.close(); sys.exit(0)'"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 45s

  client:
    container_name: smart_client
    build:
      context: .
      dockerfile: Client_dockerfile
    image: smart_client:latest
    environment:
      MCP_SERVER_URL: "http://server:8000/sse"
      OLLAMA_HOST: "http://ollama:11434"
      OLLAMA_MODEL: "llama3.1-smart-compiler:latest"
      LOG_LEVEL: "INFO"
      ALLOWED_PATHS: "/app/runtime-path"
    # Remove this block if the client doesn't expose anything to the host
    ports:
      - "8001:8001"
    volumes:
      - client_runtime:/app/runtime-path
    stdin_open: true
    tty: true
    restart: unless-stopped
    depends_on:
      server:
        condition: service_healthy
      ollama:
        condition: service_healthy
      ollama_init:
        condition: service_completed_successfully

volumes:
  ollama_data:
  server_runtime:
  client_runtime:
